---
layout: article
title: 开源与研究：我与vLLM团队的2024
key: last-chn
---

## 楔子

博士晚毕业一年是坏事么？不，是好事。

做开源项目吃力不讨好么？不，太棒了。

---------------------

看见Kaichao在知乎上的博客文章，心痒难耐我也想自己写一个，顺便记录我和vllm团队的大家一起干活的时光。

## 怎么加入的vLLM团队

在2023年中旬，我还在研究怎么压缩视频同时保留视频里面车的细节，从而在保证神经网络能数明白视频里面有多少车的同时让上传视频的过程多快好省。做这个方向做久了之后，最有创新性的想法基本都被人做过，剩下的工作基本上都是在对着不同的视频适配不同的压缩参数，总有种螺狮壳里做道场的感觉。同时，越来越多的人涌入这个方向，大家对论文的要求水涨船高，肉眼可见的越来越卷。

恰在此时，ChatGPT风靡世界，大家急需为大语言模型（LLM）提供更好的系统支持，对应的研究也开始爆发是的增长。同时，我个人平常也喜欢生成一点文字，这和大语言模型听起来就很配。所以本可以准备毕业的我决定多读一年PhD来转到大语言模型这个研究方向，同时拜托导师帮我联系暑研。

我导师作为导师的导师的关门弟子，联系上了导师的导师的开门弟子Ion Stoica（在此五体投地感谢我导师和Ion）。巧了，Ion正是创立vLLM项目的组的老师。我和vLLM团队的故事也从这里开始启航。

## 第一关：面试

在正式加入vLLM团队之前，Ion简短的面试了我。面试开始我感受到了久违的压力：一开始会议链接点不进去，我紧急排查是不是链接出错了（事后发现是Google meet的问题），然后赶紧和Ion发邮件改用zoom，然后反复刷新邮件等Ion的回复。好在最后面试总算成功开始。

其中一个让我印象深刻的问题是：描述你最骄傲的一个项目。在脑子空白之际，我在自己最有影响力的项目和最喜欢的项目（同时基本没有影响力）之间闪烁，最终凭借直觉选择讲我最喜欢的项目。在讲这个项目的过程中我文思泉涌，整个人都活了过来，最终通过了面试。

面试之后，我琢磨这个面试问题，突然发现了一种遥远的相似性：Ranveer（MSR一个组的老大）说做出最顶尖的研究需要的是和研究项目之间情感上的链接；我在讲述自己喜欢的项目时，眼睛都会放光；Haijie（Conviva程序员们的头头）在挑选人才的时候，最想看到的是motivation。

我觉得如果把这种情感上的链接/眼睛里的光/motivation找一个词来概括的话，最合适的词不是动机，而是热情。是的，做出最好的项目，需要真正的热情。这种热情只能从生活中获得，也写不进简历里面去，但是有了热情人就有了精气神。

## 初入vLLM：performance benchmark

面试在2月份完成，我以最快的速度进行各种准备，顺利在4月份到了伯克利。

不记得听谁说起过，系统研究的第一步都是做performance benchmark。所以到了vLLM团队之后，我做的第一件事就是建立一系列的performance benchmark。

在和vLLM团队的大家聊天过后，我发现做performance benchmark要展示给两类人看，背后是两种截然不同的动机和要求。

- vLLM的开发者们想看performance benchmark，目的是为了了解vLLM的性能到底有没有因为新贡献的代码变好或者变坏。这类benchmark要在每个代码变动都跑一次，不能跑太久。
- LLM从业者们想看performance benchmark，目的是为了了解vLLM和其他开源项目到底谁快谁慢。这个benchmark可以在每次有性能提升的时候发布，但是要尽可能的详细，易于理解。

于是，我说服了Ion，开始着手设计两套benchmark。

对于vLLM自己的benchmark做起来相对顺利一些。最终效果如图，里面包含了和一天前的vLLM，一周前的vLLM相比的性能变化，同时也有折线图来帮助大家定位到是哪个commit让vLLM变快变慢。

第二个benchmark是为了在vLLM和其他项目（LMDeploy, TGI, SGLang, TensorRT-LLM）之间进行尽可能公平的比较。在开始深入研究之后，才发现这个方向比我想象中的麻烦很多很多。

首先是把benchmark跑起来就就不容易，主要的复杂度来源于TensorRT-LLM。 TensorRT-LLM当时在业界公认很快也公认很难用。我参考NeuralMagic的一个demo版本的实现磕磕绊绊的跑起来，但是性能却不好。最后直到和NVIDIA的人联系上了之后才跑起来一个相对正确的版本（这个版本都和NVIDIA的内部实现比还是更慢，但我们没找出为什么）。

同时，为了保证benchmark只需要复制粘贴加回车就能自动跑起来，我把这个benchmark做到了自动测试里面。我无数次发现一行代码不对，改了一下，然后就花半个小时到一个小时等自动测试跑起来，然后跑了一会崩了研究为什么，然后再改一行代码，再花半小时，如是反复。

benchmark总算能顺利跑出结果之后，随之而来的又是有关公平性的一系列挑战：

- 没有办法限制输出长度，就算测试的是一模一样的模型，由于实现的不同，用的库的版本不同，采样方式的不同，输出长度都不一样（这个问题随着大家都实现了`--ignore-eos`之后算是有了一个答案---但这已经是很久之后了）。我调了很久TGI的输出还是莫名奇妙的不一样最后只能选择不测TGI。
- 看似一样的参数语义是不一样的（比如GPU utilization，有的是整个GPU的utilization，有的是刨除模型之后剩下GPU的utilization，有的项目会预留出来临时GPU变量的空间有的不会，很难做到彻底的对齐）。在发现这个问题之后我已经没有足够的时间来继续修复了。

最终在在vLLM团队发布性能优化博文的时候赶出了一版结果。事后我收到了来自NVIDIA和SGLang的feedback发现了一些不对劲的地方。只能说第一版benchmark的确不够好。但我尽力了。至少做到了能让大家进到docker一次复制粘贴加回车就跑的起来。我相信，只有更多的人跑起来benchmark，才能曝光问题，解决问题。

## 研究方向：分布式推理，PD分离与KV cache传输

在performance benchmark告一段落之后，我对vLLM这个软件也更加了解，开始能够偶尔蹦出来一些研究方向。但是人只有一双手，我也只能集中精力攻克一个研究，那到底应该选择什么研究方向呢？

说来有点好笑，明明知道研究最重要的就是先做起来，我却举棋不定。可能是我把这次研究看的太重了：这估计是我毕业之前能做的最后一段研究，这段研究也是我真正能横跨学界和工业界的一段研究（感谢vLLM让我能够接触到真正的LLM从业者），我也希望这个研究既能让我由衷的喜欢，又能获得工业界的影响力。

最终我选择了做KV cache传输这个方向，原因有四：

- 大势所趋：LLM越发多种多样。处理多种多样的LLM的最好方法就是把不一样的功能放在不一样的机器上。那就需要传输LLM的中间状态（也就是KV cache）。
- 工业应用：工业界已经在开始实现PD分离技术，KV cache传输正是PD分离技术的核心。
- 技术能力：我之前写过很多视频传输的论文。视频传输和KV cache传输肯定是不一样的，但是究其本质都是在传tensor。我相信之前的研究能帮助我更好的想问题。
- 个人口味：相比于建立一个复杂的系统，我更倾向建立多个简单的系统，然后优化系统间的传输和沟通来解决问题（从这点来讲虽然我在做MLSys但是内心还是个计算机网络研究者）。

带着这个念头，我开始借由PD分离这个具体的应用场景下写第一版KV cache传输的代码。意料之外（又不那么意料之外）的是，之前已经有很多版的实现，但都最终没有了下文。表面看来是因为之前的实现带来大量的代码改动，但根本原因我想是大家思维上的惯性--认为PD分离和其他的分布式推理（比如向量并行）是一样的，要进行一样的实现，要由vLLM自己来进行PD分离以及一系列的管理。这产生了两个冲突：vLLM的世界观是去优化单个模型实例的推理（而之前的工作想把Prefill和Decode的模型实例都由vLLM来管理）；PD分离强调Prefill和Decode的解耦（而之前的工作想把Prefill和Decode都写在vLLM里面，不可避免的带来耦合）。


而我的实现，通过把Prefill和Decode放在不同的vLLM实例，然后专注于实现vLLM实例之间的互联。

说来简单，这个实现前前后后花的时间远超我想象（大概2个半月），在反复和人讨论反复迭代之后我总算做出来第一版。

在做KV cache传输的第一版过程中，我们组的其他小伙伴也开始正式启动LMCache这个项目。这个项目专注于为vLLM扩展KV cache的存储空间，同时也实现了不同vLLM实例之间的KV cache互联。

这帮我补上了我的研究缺少的一块重要拼图。我的研究目的是优化KV cache传输，而优化传输的具体效果需要这个传输功能本身的实现足够优秀才能真正被展示出来.而LMCache（以及最近新开源的Mooncake）正好在优化传输功能。

也是在这个时刻，我意识到传输功能本身应该交由第三方实现，而我能做的就是为第三方实现提供一个规范的接口，同时把这个接口背后vLLM这端应该做的工作做好。至于这个KV cache传输的接口应该长什么样，又怎么和vLLM本身融合呢，我在和大家广泛的讨论（感谢vLLM团队，Mooncake，SGLang，NVIDIA在这个过程中提供的宝贵意见），希望最终能够交出一个让我满意，也能让大家满意的答卷。我们拭目以待。


## 后记：为什么要做开源

世界上99%的博士们是不会花大力气在做开源上的。那为什么Ion的组（以及vLLM团队的大家）选择看似吃力不讨好和论文发表没直接关系的开源项目上呢？

我不清楚Ion是怎么想的，这里只是我个人的看法。系统研究是被真实世界的应用驱动的研究。真正顶尖的系统研究一定夹在工程和科研的中间。只有在工程里面够久，才能拥有顶尖的技术，才能第一时间感知到现在工业界里面什么东西重要；只有带着科研的视角，才能看到什么问题有科研的价值，什么问题的解法背后有根本性的观念转变，而不是满足于只是把问题解决。

而只有开源，才会有人愿意帮你解决在你的想法真正落地的过程中的一系列工程挑战。虽然开源会牵扯很多的精力，但是我看不到更好的方法。