---
layout: article
title: 做开源的研究：我与vllm团队
key: last-chn
---

看见Kaichao的post，心痒难耐我也想自己写一个，就当是记录我和vllm团队的大家一起干活的时光，也给我博士生涯最后一段路留点文字纪念。

## 怎么加入的vLLM团队

和Kaichao的经历相似，我也是为了换一个研究方向来到vllm团队的。博士前四年半我主要在研究怎么压缩视频同时保留视频里面车的细节，从而在保证神经网络能数明白视频里面有多少车的同时让上传视频的过程多快好省。这个方向做到后面，最有创新性的想法基本都被人做过，剩下的工作基本上都是在对着不同的视频适配不同的压缩参数，总有种螺狮壳里做道场的感觉。恰在此时，随着ChatGPT风靡世界，为大语言模型（LLM）提供系统支持的研究也风靡全球。我个人平常也喜欢生成一点文字，这和大语言模型听起来就很配。所以我决定转研究方向，同时拜托导师帮我联系暑研。

感谢导师帮我联系到了Ion Stoica组里的暑研。我和vllm团队的故事也从这里开始启航。

## 第一关：面试

在正式加入vllm团队之前，Ion简短的面试了我。其中一个让我印象深刻的问题是：描述你最骄傲的一个项目。我在自己最有影响力的项目和最喜欢的项目（同时基本没有影响力）之间犹豫了好几秒，最后选择了我最喜欢的工作。在讲这个项目的过程中我整个人都活了过来，最终通过了面试。

面试之后，我琢磨这个面试问题，突然发现了一种遥远的相似性：Ranveer说做出最顶尖的研究需要的是和研究项目之间情感上的链接；Haijie在挑选人才的时候，面试官们挑选人才的时候，最想看到的是motivation。

我觉得如果把motivation翻译成中文的话，最合适的词不是动机，而是热情。是的，做出最好的项目，需要真正的热情。热情写不进简历里面去，但是有了热情人就有了精气神。

## 初入vLLM：performance benchmark

不记得听谁说起过，系统研究的第一步都是做performance benchmark。所以到了vLLM团队之后，我做的第一件事就是建立一系列的performance benchmark，同时尽可能做到和其他项目（LMDeploy, TGI, SGLang, TensorRT-LLM）进行公平的比较。在开始深入研究之后，才发现这个方向比我想象中的麻烦许多：首先是把benchmark跑起来的两个挑战：

- TensorRT-LLM当时在业界公认很快也公认很难用，最后直到和NVIDIA的人联系上了之后才跑起来一个相对正确的版本（这个版本都和NVIDIA的内部实现比还是更慢，但是我们没找出为什么）。
- 为了让benchmark能够让人只需要复制粘贴加回车就能自动跑起来，要一遍一遍的改，同时为了保证让每次的benchmark环境公平，还要和infra做斗争（斗争一次半小时就过去了）

随之而来的又是有关公平性的一系列挑战：

- 没有办法限制输出长度，就算测试的是一模一样的模型，由于实现的不同，用的库的版本不同，采样方式的不同，输出长度都不一样（这个问题随着大家都实现了`--ignore-eos`之后算是有了一个答案---但这已经是很久之后了）。我调了很久TGI的输出还是莫名奇妙的不一样最后只能选择不测TGI。
- 看似一样的参数语义是不一样的（比如gpu utilization，有的是整个GPU的utilization，有的是刨除模型之后剩下GPU的utilization，有的项目会预留出来临时GPU变量的空间有的不会，很难做到彻底的对齐）。在发现这个问题之后我已经没有足够的时间来继续修复了。

最终在在vLLM团队发布性能优化博文的时候赶出了一版结果。事后我收到了来自NVIDIA和SGLang的feedback发现了一些不对劲的地方，只能说不完美，但是尽力了。至少做到了能让大家进到docker一次复制粘贴加回车就跑的起来。我相信，benchmark只有能让更多的人跑起来，这些问题才能得到曝光和解决。

## 研究方向：分布式推理，PD分离与KV cache传输

在performance benchmark告一段落之后，我开始