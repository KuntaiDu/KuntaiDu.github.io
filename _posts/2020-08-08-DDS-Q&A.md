---
layout: article
title: DDS Q&A
key: DDS-QA
---

# Delay issue

## Will your approach has higher latency?

First, DDS needs to buffer frames at the camera since DDS encodes the video into video segments. So DDS has higher response delay than those frame-based camera-side heuristics.
However, when we compare DDS with the remaining methods, we find that DDS generates about 90% of the inference results before the iteration starts. This part of results are derived from low-quality video, so the streaming delay on the network is actually lower than the baselines. So in average, the response delay is actually lower than the baseline.

## Why average response delay?

We agree that average responses delay is not a good metric for those applications that are sensitive to worst-time delay, like counting the number of cars on every frame. But average response delay is meaningful if we can directly update the analytic metric after generating a new inference results. One example is calculating the average speed of cars on a road). 

# Compute cost issue

## You are actually trading compute cost to bandwidth cost. Is that a worthwhile trade?

It depends on which part of cost is dominant. For example, if the camera is connected through ethernet cable, DDS might be sub-optimal. But DDS is a good solution if the camera is connected to a personal cluster through LTE.

# Rationale

## Why DDS can propose more regions than the inference results?

The fundamental reason is that 
generating regions that contain objects is a binary classificaton task, and
is easier than gnerating regions that contain specific types of objects, which is a multi-class classification.
So we can generate more regions that contain objects than region that contain specific types of objects. These regions are exactly what DDS is looking for.

## Why video streaming for analytics?

The short answer is that video streaming for analytics is typically more cheap. The reason is that accurate deep-neural-net-based analytic algorithms require the support of high-end GPU. But putting the GPU at the camera is way more expensive than putting the GPU at the server. There are a lot of reasons for this, like the camera-side GPU cannot be reused by other video streams and other tasks, the GPU also occupies a lot of space in the camera and requires powerful heat dissipation and so on.

# Questions from the preview video

## How might this solution change if you could run more computation at the sender?

Good question. My intuition is that it depends on the complexity of the content. When the video content is not challenging, like in home camera, the accuracy of camera-side heuristics may be pretty high and usable. But when the video content is really challenging, like in drone camera, the accuracy of camera-side heuristics is low and unreliable. In this case, we may want to stick with our original solution.

## How does DDS balance sender-side and receiver-side computation compare with prior work?

The bandwidth also plays an important role here. 

DDS requires more compute on the server and require little compute on the camera. This is a good trade when the 